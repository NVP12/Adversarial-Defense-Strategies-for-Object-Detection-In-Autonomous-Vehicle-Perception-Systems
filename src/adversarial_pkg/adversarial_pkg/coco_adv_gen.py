# -*- coding: utf-8 -*-
"""coco_adv_gen.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EvFR24BNwD3ansmzLnK7raAo6zrMTb9f
"""

!unzip /content/annotations_trainval2017.zip -d /content/annotations/

!git clone https://github.com/KaranJagtiani/YOLO-Coco-Dataset-Custom-Classes-Extractor.git

!cp /content/annotations/annotations/instances_train2017.json /content/YOLO-Coco-Dataset-Custom-Classes-Extractor/

!cd /content/YOLO-Coco-Dataset-Custom-Classes-Extractor && ls && python coco-extractor.py --help

!cd /content/YOLO-Coco-Dataset-Custom-Classes-Extractor &&  python coco-extractor.py "person" "bicycle" "car" "motorcycle" "bus" "train" "truck" "boat" "traffic light" "fire hydrant" "stop sign" "parking meter"

!apt-get install rsync -y

!pip install ultralytics

"""#FGSM"""

import torch
import torchvision.transforms as T
from torch.utils.data import DataLoader
from torchvision.datasets import CocoDetection
from ultralytics import YOLO
from tqdm import tqdm
import os
from PIL import Image
import numpy as np
from pycocotools.coco import COCO

# ========== CONFIG ==========
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
EPS = 8 / 255

ROOT = "/content"
IMG_DIR = os.path.join(ROOT, "YOLO-Coco-Dataset-Custom-Classes-Extractor/downloaded_images")  # your subset of images
ANN_FILE = os.path.join(ROOT, "annotations/annotations", "instances_train2017.json")

SAVE_IMG_DIR = "/content/adv_output_with_gt/images"
SAVE_LABEL_DIR = "/content/adv_output_with_gt/labels"

os.makedirs(SAVE_IMG_DIR, exist_ok=True)
os.makedirs(SAVE_LABEL_DIR, exist_ok=True)

# ========== MODEL ==========
model = YOLO("yolov8l.pt").to(DEVICE).eval()

# ========== DATA ==========
transform = T.Compose([
    T.Resize((640, 640)),
    T.ToTensor()
])


class CocoDetectionWithCheck(CocoDetection):
    def __getitem__(self, idx):
        img_id = self.ids[idx]
        img_name = f"{img_id:012d}.jpg"
        img_path = os.path.join(self.root, img_name)
        if not os.path.exists(img_path):
            print(f"Image {img_name} not found, skipping.")
            return None, None
        img, target = super(CocoDetectionWithCheck, self).__getitem__(idx)
        return img, target

dataset = CocoDetectionWithCheck(root=IMG_DIR, annFile=ANN_FILE, transform=transform)


# Load COCO API to get category ID mapping
coco_api = COCO(ANN_FILE)
cat_ids = coco_api.getCatIds()
cat_id_to_yolo = {cat_id: idx for idx, cat_id in enumerate(cat_ids)}

def collate_fn(batch):
    batch = [b for b in batch if b is not None and b[0] is not None and b[1] is not None]
    if len(batch) == 0:
        return None, None
    imgs, targets = zip(*batch)
    return torch.stack(imgs), targets

loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

"""3"""

# ========== ATTACK LOOP ==========
for i, (imgs, targets) in tqdm(enumerate(loader), total=len(dataset), desc="Generating adversarial examples"):
    # Skip if image or targets are None (missing image)
    if imgs is None or targets is None:
        continue  # Skip to next iteration if image is missing

    imgs = imgs.to(DEVICE).requires_grad_(True)

    # Run YOLO and get gradients w.r.t objectness
    preds = model.model(imgs)
    obj_conf = preds[..., 4]
    loss = -obj_conf.sum()
    model.model.zero_grad()
    loss.backward()

    # FGSM step
    adv_imgs = (imgs + EPS * imgs.grad.sign()).clamp(0, 1).detach()

    # Get original image id
    img_id = dataset.ids[i]
    img_name = f"{img_id:012d}.jpg"
    label_name = img_name.replace('.jpg', '.txt')

    # Save adversarial image
    img_pil = T.ToPILImage()(adv_imgs[0].cpu())
    img_pil.save(os.path.join(SAVE_IMG_DIR, img_name))

    # === Save original labels (with category mapping) ===
    original_boxes = targets[0]  # COCO format
    with open(os.path.join(SAVE_LABEL_DIR, label_name), 'w') as f:
        for obj in original_boxes:
            coco_cat = obj["category_id"]
            cls = cat_id_to_yolo[coco_cat]  # map to YOLO class index

            bbox = obj["bbox"]  # [x_min, y_min, width, height]
            x_min, y_min, w, h = bbox
            cx = x_min + w / 2
            cy = y_min + h / 2

            cx /= 640  # resized image
            cy /= 640
            w /= 640
            h /= 640

            f.write(f"{cls} {cx} {cy} {w} {h}\n")

    torch.cuda.empty_cache()

!pwd



import os

def get_all_downloaded_image_ids(root_dir):
    image_ids = set()
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".jpg"):
                try:
                    image_id = int(os.path.splitext(file)[0])
                    image_ids.add(image_id)
                except ValueError:
                    continue  # Skip if filename is not a valid COCO numeric ID
    return image_ids

# Define your actual path
downloaded_images_root = "/content/YOLO-Coco-Dataset-Custom-Classes-Extractor/downloaded_images"

# Call the function
downloaded_image_ids = get_all_downloaded_image_ids(downloaded_images_root)

# Print the count
print(f"Number of downloaded COCO images: {len(downloaded_image_ids)}")



"""fgsm 2"""

from PIL import Image
from torchvision import transforms
from tqdm import tqdm
from pathlib import Path
import shutil

# --- CONFIGURATION ---
EPSILON = 8 / 255
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'

INPUT_DIR = Path("/content/YOLO-Coco-Dataset-Custom-Classes-Extractor/downloaded_images")
OUTPUT_IMG_DIR = Path("/content/adversarial_samples/images")
OUTPUT_TXT_DIR = Path("/content/adversarial_samples/labels")

# Ensure output dirs exist
OUTPUT_IMG_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_TXT_DIR.mkdir(parents=True, exist_ok=True)

# Image transform
transform = transforms.Compose([
    transforms.Resize((640, 640)),
    transforms.ToTensor()
])

# Load YOLOv8
model = YOLO("yolov8l.pt").model.to(DEVICE).eval()

# Recursively gather all .jpg + .txt pairs
image_txt_pairs = []
for root, _, files in os.walk(INPUT_DIR):
    for file in files:
        if file.endswith(".jpg"):
            img_path = Path(root) / file
            txt_path = img_path.with_suffix(".txt")
            if txt_path.exists():
                image_txt_pairs.append((img_path, txt_path))

print(f"Found {len(image_txt_pairs)} image-txt pairs.")

# FGSM Attack and Save Function
def process_image(img_path, txt_path):
    try:
        image = Image.open(img_path).convert("RGB")
        tensor = transform(image).unsqueeze(0).to(DEVICE).requires_grad_(True)

        # Forward pass
        output = model(tensor)
        preds = output[0] if isinstance(output, tuple) else output
        obj_conf = preds[..., 4]
        loss = -obj_conf.sum()
        model.zero_grad()
        loss.backward()

        # FGSM attack
        adv_tensor = (tensor + EPSILON * tensor.grad.sign()).clamp(0, 1).squeeze(0).cpu()
        adv_image = transforms.ToPILImage()(adv_tensor)

        # Save adversarial image
        out_img = OUTPUT_IMG_DIR / img_path.name
        out_txt = OUTPUT_TXT_DIR / txt_path.name
        adv_image.save(out_img)

        # Copy original annotation
        shutil.copy(txt_path, out_txt)

    except Exception as e:
        print(f"Error on {img_path.name}: {e}")

# Generate and save all
for img_path, txt_path in tqdm(image_txt_pairs, desc="Generating adversarial samples"):
    process_image(img_path, txt_path)

print("âœ… All adversarial samples generated and saved.")

"""pgd2"""

import os
from pathlib import Path
from PIL import Image
from torchvision import transforms
import torch
from ultralytics import YOLO
from tqdm import tqdm
import shutil

# --- CONFIGURATION ---
EPSILON = 8 / 255
ALPHA = 2 / 255
ITERATIONS = 10
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
BATCH_SIZE = 8

INPUT_DIR = Path("/content/YOLO-Coco-Dataset-Custom-Classes-Extractor/downloaded_images")
OUTPUT_IMG_DIR = Path("/content/pgd_samples/images")
OUTPUT_TXT_DIR = Path("/content/pgd_samples/labels")

transform = transforms.Compose([
    transforms.Resize((640, 640)),
    transforms.ToTensor()
])

# Load YOLOv8 model
yolo_model = YOLO("yolov8n.pt").model.to(DEVICE).eval()

def pgd_attack(images, model, epsilon, alpha, iters):
    original = images.clone().detach()
    adv_images = images.clone().detach()
    for _ in range(iters):
        adv_images.requires_grad = True
        outputs = model(adv_images)
        preds = outputs[0] if isinstance(outputs, tuple) else outputs
        obj_conf = preds[..., 4]
        loss = -obj_conf.sum()
        model.zero_grad()
        loss.backward()
        grad = adv_images.grad.sign()
        adv_images = adv_images + alpha * grad
        adv_images = torch.max(torch.min(adv_images, original + epsilon), original - epsilon)
        adv_images = adv_images.clamp(0, 1).detach()
    return adv_images

def collect_image_txt_pairs():
    pairs = []
    for root, _, files in os.walk(INPUT_DIR):
        for file in files:
            if file.endswith(".jpg"):
                img_path = Path(root) / file
                txt_path = img_path.with_suffix(".txt")
                if txt_path.exists():
                    pairs.append((img_path, txt_path))
    return pairs

def load_batch(batch_paths):
    imgs, img_files, txt_files = [], [], []
    for img_path, txt_path in batch_paths:
        try:
            image = Image.open(img_path).convert("RGB")
            tensor = transform(image)
            imgs.append(tensor)
            img_files.append(img_path)
            txt_files.append(txt_path)
        except Exception as e:
            print(f"Error loading {img_path.name}: {e}")
    if not imgs:
        return None, [], []
    return torch.stack(imgs).to(DEVICE), img_files, txt_files

def save_batch(adv_batch, img_paths, txt_paths):
    OUTPUT_IMG_DIR.mkdir(parents=True, exist_ok=True)
    OUTPUT_TXT_DIR.mkdir(parents=True, exist_ok=True)
    for i in range(len(img_paths)):
        adv_img = transforms.ToPILImage()(adv_batch[i].cpu())
        adv_img.save(OUTPUT_IMG_DIR / img_paths[i].name)
        shutil.copy(txt_paths[i], OUTPUT_TXT_DIR / txt_paths[i].name)

# --- MAIN EXECUTION ---

# Already processed images
existing_images = {f.name for f in OUTPUT_IMG_DIR.glob("*.jpg")}
image_txt_pairs = [
    (img, txt) for img, txt in collect_image_txt_pairs()
    if img.name not in existing_images
]

print(f"Remaining to process: {len(image_txt_pairs)}")

# Process in batches
for i in tqdm(range(0, len(image_txt_pairs), BATCH_SIZE), desc="Generating PGD samples"):
    batch = image_txt_pairs[i:i + BATCH_SIZE]
    if not batch:
        continue
    imgs_tensor, img_paths, txt_paths = load_batch(batch)
    if imgs_tensor is None:
        continue
    adv_imgs = pgd_attack(imgs_tensor, yolo_model, EPSILON, ALPHA, ITERATIONS)
    save_batch(adv_imgs, img_paths, txt_paths)

print(f"\nâœ… Finished generating PGD adversarial samples.")

!zip -r fgsm_data.zip /content/adversarial_samples > /dev/null
from google.colab import files
files.download("/content/fgsm_data.zip")

!zip -r pgd_data.zip /content/pgd_samples > /dev/null

!cp /content/pgd_data.zip /content/drive/MyDrive/

!rsync -ah --progress pgd_data.zip /content/drive/MyDrive/

!rsync -ah --progress original_data.zip /content/drive/MyDrive/



import os

def get_all_downloaded_image_ids(root_dir):
    image_ids = set()
    for subdir, _, files in os.walk(root_dir):
        for file in files:
            if file.endswith(".jpg"):
                try:
                    image_id = int(os.path.splitext(file)[0])
                    image_ids.add(image_id)
                except ValueError:
                    continue  # Skip if filename is not a valid COCO numeric ID
    return image_ids

# Define your actual path
downloaded_images_root = "/content/pgd_samples/images"

# Call the function
downloaded_image_ids = get_all_downloaded_image_ids(downloaded_images_root)

# Print the count
print(f"Number of downloaded COCO images: {len(downloaded_image_ids)}")

!unzip /content/annotations_trainval2017.zip -d /content/annotations/

from google.colab import drive
drive.mount('/content/drive')

path_FGSM = '/content/drive/My Drive/DL/fgsm_data.zip'

!unzip '/content/drive/My Drive/DL/fgsm_data.zip' -d /content/FGSM_data/

!unzip '/content/drive/My Drive/DL/original_data.zip' -d /content/original_data/
!unzip '/content/drive/My Drive/DL/pgd_data.zip' -d /content/pgd_data/

from pathlib import Path
import random, shutil, os, torch, PIL
from torchvision.transforms import (
    Compose, Resize, ToTensor, Normalize
)
from torch.utils.data import Dataset, DataLoader, random_split

# root dirs
CLEAN_DIR = Path('/content/original_data')
FGSM_DIR  = Path('/content/FGSM_data')
PGD_DIR   = Path('/content/pgd_data')

assert CLEAN_DIR.is_dir() and FGSM_DIR.is_dir() and PGD_DIR.is_dir()

def all_imgs(root):
    return sorted([p for p in root.rglob('*') if p.suffix.lower() in {'.jpg','.jpeg','.png'}])

clean_files = all_imgs(CLEAN_DIR)
adv_files   = all_imgs(FGSM_DIR) + all_imgs(PGD_DIR)   # FGSM + PGD together

files  = clean_files + adv_files
labels = [0]*len(clean_files) + [1]*len(adv_files)     # 0=clean, 1=adv

print(f'Clean:{len(clean_files)}  Adv:{len(adv_files)}  Total:{len(files)}')

IMG_SIZE = 128
mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]

tf = Compose([
    Resize((IMG_SIZE, IMG_SIZE)),
    ToTensor(),
    Normalize(mean, std),
])

class BinaryImgDataset(Dataset):
    def __init__(self, paths, labs, transform):
        self.paths, self.labs, self.transform = paths, labs, transform
    def __len__(self):           return len(self.paths)
    def __getitem__(self, idx):
        img = PIL.Image.open(self.paths[idx]).convert('RGB')
        return self.transform(img), self.labs[idx]

full_ds = BinaryImgDataset(files, labels, tf)

VAL_FRAC = 0.2
val_len  = int(len(full_ds)*VAL_FRAC)
train_ds, val_ds = random_split(full_ds, [len(full_ds)-val_len, val_len],
                                generator=torch.Generator().manual_seed(42))

BATCH = 64
train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=4, pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)

# ======================  MagNet training cell  =========================
import torch, torch.nn as nn, torch.optim as optim
from sklearn.metrics import roc_auc_score
import numpy as np, math, time, os

# ---------- 1.  model definition ----------
class MagNetDetector(nn.Module):
    """
    Lightweight binary classifier: 0 = clean, 1 = adversarial.
    """
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 16, 3, padding=1), nn.BatchNorm2d(16), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)          # â†’ (B,64,1,1)
        )
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(64, 1)                 # 1 logit
        )
    def forward(self, x):
        x = self.features(x).flatten(1)
        return torch.sigmoid(self.classifier(x))     # p(adv) in [0,1]


# ---------- 2.  training config ----------
device   = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model    = MagNetDetector().to(device)
opt      = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)
criterion = nn.BCELoss()

EPOCHS   = 10
IMG_SIZE = 128                     # same as you used in transforms
mean, std = [0.485,0.456,0.406], [0.229,0.224,0.225]

best_auc = 0.0
start    = time.time()

# ---------- 3.  train / validate ----------
for epoch in range(1, EPOCHS+1):

    # ---- train ----
    model.train()
    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.float().unsqueeze(1).to(device)
        pred   = model(xb)
        loss   = criterion(pred, yb)
        opt.zero_grad(); loss.backward(); opt.step()

    # ---- validate ----
    model.eval()
    preds, labs = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            p = model(xb.to(device)).cpu().numpy()
            preds.append(p)
            labs.append(yb.numpy().reshape(-1,1))
    auc = roc_auc_score(np.vstack(labs), np.vstack(preds))

    print(f'Epoch {epoch:02d} | Val AUROC: {auc:.4f}')

    # ---- checkpoint ----
    if auc > best_auc:
        best_auc = auc
        ckpt = {
            'model_state': model.state_dict(),
            'input_size':  (3, IMG_SIZE, IMG_SIZE),
            'normalization': {'mean': mean, 'std': std}
        }
        torch.save(ckpt, 'best_detector.pt')
        print('   â†³ âœ“ saved new best model')

print(f'Done in {(time.time()-start)/60:.1f} min â€“ best AUROC {best_auc:.4f}')
# ======================================================================